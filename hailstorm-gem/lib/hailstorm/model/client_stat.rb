require 'nokogiri'

require 'hailstorm/model'
require 'hailstorm/model/execution_cycle'
require 'hailstorm/model/jmeter_plan'
require 'hailstorm/model/page_stat'
require 'hailstorm/model/jtl_file'
require 'hailstorm/support/quantile'
require 'hailstorm/support/collection_helper'

# Statistics collected on the 'client' side.
# @author Sayantam Dey
# TODO Rubocop fixes
class Hailstorm::Model::ClientStat < ActiveRecord::Base
  include Hailstorm::Support::CollectionHelper

  belongs_to :execution_cycle

  belongs_to :jmeter_plan

  belongs_to :clusterable, polymorphic: true

  has_many :page_stats, dependent: :destroy

  has_many :jtl_files, dependent: :delete_all

  # starting (minimum) timestamp of collected samples
  attr_accessor :start_timestamp

  # last sample collected
  attr_accessor :end_sample

  # Array to store 90% response time of all samples
  attr_accessor :sample_response_times

  after_initialize :set_defaults

  # Collects the statistics generated by the load generating cluster
  # @param [Hailstorm::Model::ExecutionCycle] execution_cycle
  # @param [Hailstorm::Behavior::Clusterable] cluster_instance
  def self.collect_client_stats(execution_cycle, cluster_instance)
    logger.debug { "#{self.class}.#{__method__}" }
    jmeter_plan_results_map = Hash.new { |h, k| h[k] = [] }
    result_mutex = Mutex.new
    local_log_path = File.join(Hailstorm.root, Hailstorm.log_dir)

    self.visit_collection(cluster_instance.master_agents.where(active: true)) do |master|
      result_file_name = master.result_for(self, local_log_path)
      result_file_path = File.join(local_log_path, result_file_name)
      result_mutex.synchronize do
        jmeter_plan_results_map[master.jmeter_plan_id].push(result_file_path)
      end
    end

    jmeter_plan_results_map.keys.sort.each do |jmeter_plan_id|
      stat_file_paths = jmeter_plan_results_map[jmeter_plan_id]
      self.create_client_stat(execution_cycle, jmeter_plan_id, cluster_instance, stat_file_paths)
      stat_file_paths.each { |file_path| File.unlink(file_path) }
    end
  end

  # create 1 record for client_stats if it does not exist yet
  # @param [Hailstorm::Model::ExecutionCycle] execution_cycle
  # @param [Integer] jmeter_plan_id
  # @param [Hailstorm::Behavior::Clusterable] clusterable
  # @param [Array<String>] stat_file_paths
  # @param [Boolean] rm_stat_file
  # @return [[Hailstorm::Model::ClientStat, String]] [client_stat, combined_file_path: nil]
  def self.create_client_stat(execution_cycle, jmeter_plan_id, clusterable, stat_file_paths)
    jmeter_plan = Hailstorm::Model::JmeterPlan.find(jmeter_plan_id)
    template = ClientStatTemplate.new(jmeter_plan, execution_cycle, clusterable, stat_file_paths)
    template.logger = logger
    template.create
  end

  # Generates a hits per second graph
  def self.hits_per_second_graph(execution_cycle, width: 640, height: 300, builder: nil)
    sax_document = ResponseTimeFreqDist.new
    sax_parser = Nokogiri::XML::SAX::Parser.new(sax_document)
    execution_cycle.client_stats.each do |client_stat|
      export_file = client_stat.write_jtl(Hailstorm.tmp_path, true)
      File.open(export_file, 'r') do |file|
        sax_parser.parse(file)
      end
    end

    grapher = GraphBuilderFactory.time_series_graph(series_name: 'Requests/second',
                                                    range_name: 'Requests',
                                                    start_time: sax_document.start_time,
                                                    other_builder: builder)
    sax_document.hit_matrix.each do |key, value|
      grapher.addDataPoint(key, value)
    end

    output_path = File.join(Hailstorm.root, Hailstorm.reports_dir, "hits_per_second_graph_#{execution_cycle.id}")
    grapher.build(output_path, width, height)
  end

  def self.active_threads_over_time_graph(execution_cycle, width: 640, height: 300, builder: nil)
    sax_document = VirtualUserTimeDist.new
    sax_parser = Nokogiri::XML::SAX::Parser.new(sax_document)

    execution_cycle.client_stats.each do |client_stat|
      export_file = client_stat.write_jtl(Hailstorm.tmp_path, true)
      File.open(export_file, 'r') do |file|
        sax_parser.parse(file)
      end
    end

    grapher = GraphBuilderFactory.time_series_graph(series_name: 'Virtual Users / Second',
                                                    range_name: 'Virtual Users',
                                                    start_time: sax_document.start_time,
                                                    other_builder: builder)

    ts_keys = sax_document.vusers_matrix.keys.sort_by(&:to_i)
    ts_keys.each_with_index do |ts, index|
      previous_index = index > 1 ? index - 1 : index
      next_index = index < ts_keys.size - 1 ? index + 1 : index
      previous_count = sax_document.vusers_matrix[ts_keys[previous_index]]
      threads_count = sax_document.vusers_matrix[ts]
      next_count = sax_document.vusers_matrix[ts_keys[next_index]]
      if (previous_count == next_count) && (previous_count > threads_count)
        threads_count = previous_count # dip correction
      end
      grapher.addDataPoint(ts, threads_count)
    end

    output_path = File.join(Hailstorm.root, Hailstorm.reports_dir, "vusers_per_second_graph_#{execution_cycle.id}")
    grapher.build(output_path, width, height)
  end

  def self.throughput_over_time_graph(execution_cycle, width: 640, height: 300, builder: nil)
    sax_document = ThroughputTimeDist.new
    sax_parser = Nokogiri::XML::SAX::Parser.new(sax_document)
    execution_cycle.client_stats.each do |client_stat|
      export_file = client_stat.write_jtl(Hailstorm.tmp_path, true)
      File.open(export_file, 'r') do |file|
        sax_parser.parse(file)
      end
    end

    grapher = GraphBuilderFactory.time_series_graph(series_name: 'Throughput over time',
                                                    range_name: 'Bytes Transferred',
                                                    start_time: sax_document.start_time,
                                                    other_builder: builder)
    sax_document.byte_matrix.each do |key, value|
      grapher.addDataPoint(key, value)
    end

    output_path = File.join(Hailstorm.root, Hailstorm.reports_dir, "throughput_per_second_graph_#{execution_cycle.id}")
    grapher.build(output_path, width, height)
  end

  # @return [String] path to generated image
  def aggregate_graph(builder: nil)
    page_labels = self.page_stats.collect(&:page_label)
    threshold_data, threshold_titles = build_threshold_matrix
    error_percentages = self.page_stats.collect(&:percentage_errors)
    grapher = GraphBuilderFactory.aggregate_graph(identifier: self.id, other_builder: builder)
    grapher.setPages(page_labels)
           .setNinetyCentileResponseTimes(response_time_matrix[3])
           .setThresholdTitles(threshold_titles)
           .setThresholdData(threshold_data)
           .setErrorPercentages(error_percentages)
           .create
  end

  def aggregate_stats
    self.page_stats.collect(&:stat_item)
  end

  def collect_sample(sample)
    sample_timestamp = sample['ts'].to_f

    # start_timestamp
    self.start_timestamp = sample_timestamp if self.start_timestamp.nil? || (sample_timestamp < self.start_timestamp)

    # end_sample
    self.end_sample = sample if self.end_sample.nil? || (sample_timestamp > self.end_sample['ts'].to_f)

    self.sample_response_times.push(sample['t'])
  end

  # @param [String] export_dir directory for exported files
  # @return [String] path to exported file
  def write_jtl(export_dir, append_id = false)
    require(self.clusterable_type.underscore)
    file_name = [self.clusterable.slug.gsub(/[\W\s]+/, '_'),
                 self.jmeter_plan.test_plan_name.gsub(/[\W\s]+/, '_')].join('-')
    file_name.concat("-#{self.id}") if append_id
    file_name.concat('.jtl')

    export_file = File.join(export_dir, file_name)
    Hailstorm::Model::JtlFile.export_file(self, export_file) unless File.exist?(export_file)
    export_file
  end

  def first_sample_at
    Time.at((self.start_timestamp / 1000).to_i) if self.start_timestamp
  end

  # Receives event callbacks as XML is parsed
  class JtlDocument < Nokogiri::XML::SAX::Document
    attr_reader :page_stats_map

    def initialize(stat_klass, client_stat)
      @stat_klass = stat_klass
      @client_stat = client_stat
      @page_stats_map = {}
      @level = 0
    end

    # @overrides Nokogiri::XML::SAX::Document#start_element()
    # @param [String] name
    # @param [Array] attrs
    def start_element(name, attrs = [])
      # check for level 1 because we don't collect sub-samples
      if (@level == 1) && %w[httpSample sample].include?(name)
        attrs_map = Hash[attrs] # convert array of 2 element arrays to Hash
        label = attrs_map['lb'].strip
        unless @page_stats_map.key?(label)
          @page_stats_map[label] = @stat_klass.new(page_label: label, client_stat_id: @client_stat.id)
        end
        @client_stat.collect_sample(attrs_map)
        @page_stats_map[label].collect_sample(attrs_map)
      end
      @level += 1
    end

    # @overrides Nokogiri::XML::SAX::Document#end_element()
    def end_element(_name)
      @level -= 1
    end
  end

  # Template for creating client_stat
  class ClientStatTemplate
    attr_reader :jmeter_plan, :execution_cycle, :clusterable, :stat_file_paths
    attr_accessor :logger

    def initialize(new_jmeter_plan, new_execution_cycle, new_clusterable, new_stat_file_paths)
      @jmeter_plan = new_jmeter_plan
      @execution_cycle = new_execution_cycle
      @clusterable = new_clusterable
      @stat_file_paths = new_stat_file_paths
    end

    # @return [[Hailstorm::Model::ClientStat, String]] [client_stat, combined_file_path: nil]
    def create
      stat_file_path, combined = collate_stats
      client_stat = nil
      File.open(stat_file_path, 'r') do |file_io|
        client_stat = do_create_client_stat(file_io)
      end

      persist_jtl(client_stat, stat_file_path)

      tuple = [client_stat]
      tuple.push(stat_file_path) if combined
      tuple
    end

    private

    def collate_stats
      stat_file_path = stat_file_paths.first if stat_file_paths.size == 1
      stat_file_path ||= combine_stats
      [stat_file_path, stat_file_paths.size > 1]
    end

    # Combines two or more JTL files to create new JTL file with combined stats.
    def combine_stats
      xml_decl = '<?xml version="1.0" encoding="UTF-8"?>'
      test_results_start_tag = '<testResults version="1.2">'
      test_results_end_tag = '</testResults>'
      file_unique_ids = [execution_cycle, jmeter_plan, clusterable].compact.map(&:id)
      combined_file_path = File.join(Hailstorm.tmp_path, "results-#{file_unique_ids.join('-')}-all.jtl")

      File.open(combined_file_path, 'w') do |combined_file|
        combined_file.puts xml_decl
        combined_file.puts test_results_start_tag

        stat_file_paths.each do |file_path|
          File.open(file_path, 'r') do |file|
            file.each_line do |line|
              if line[xml_decl].nil? && line[test_results_start_tag].nil? && line[test_results_end_tag].nil?
                combined_file.print(line)
              end
            end
          end
        end

        combined_file.puts test_results_end_tag
      end

      combined_file_path
    end

    def do_create_client_stat(io_like)
      client_stat = execution_cycle
                    .client_stats
                    .where(jmeter_plan_id: jmeter_plan.id,
                           clusterable_id: clusterable.id,
                           clusterable_type: clusterable.class.name,
                           threads_count: jmeter_plan.latest_threads_count)
                    .first_or_create!

      # SAX parsing
      jtl_document = JtlDocument.new(Hailstorm::Model::PageStat, client_stat)
      jtl_parser = Nokogiri::XML::SAX::Parser.new(jtl_document)
      jtl_parser.parse(io_like)

      # save in db
      jtl_document.page_stats_map.values.each(&:save!)

      # update aggregates
      aggregate_samples_count = client_stat.page_stats.sum(:samples_count)
      test_duration = (client_stat.end_sample['ts'].to_f +
          client_stat.end_sample['t'].to_f - client_stat.start_timestamp) / 1000.to_f

      client_stat.aggregate_response_throughput = (aggregate_samples_count.to_f / test_duration)

      logger.debug { 'Calculating aggregate_ninety_percentile...' } if logger
      client_stat.aggregate_ninety_percentile = client_stat.sample_response_times.quantile(90)
      logger.debug { '... finished calculating aggregate_ninety_percentile' } if logger

      # this is the duration of the last sample sent, it is in milliseconds, so
      # we divide it by 1000
      sample_duration = (client_stat.end_sample['ts'].to_i + client_stat.end_sample['t'].to_i)
      client_stat.last_sample_at = Time.at(sample_duration / 1000)

      client_stat.save!
      client_stat
    end

    # persist file to db and remove file from fs
    def persist_jtl(client_stat, stat_file_path)
      logger.info { "Persisting #{stat_file_path} to DB..." } if logger
      Hailstorm::Model::JtlFile.persist_file(client_stat, stat_file_path)
    end
  end

  # SAX document that calculates a response time frequency distribution
  class ResponseTimeFreqDist < Nokogiri::XML::SAX::Document
    attr_reader :hit_matrix
    attr_reader :start_time

    def start_document
      @level = 0
      @hit_matrix = {} if @hit_matrix.nil?
    end

    def start_element(name, attrs = [])
      return unless %w[httpSample sample].include?(name)

      @level += 1
      attrs_map = Hash[attrs]
      tms = attrs_map['ts'].to_i # ms
      ts = tms / 1000 # sec
      @hit_matrix[ts] = @hit_matrix[ts].to_i + 1 if @parent_ts.nil? || (@parent_ts != ts)
      @parent_ts = ts if @level == 1
      @start_time = tms if @start_time.nil? || (@start_time > tms)
    end

    def end_element(name)
      return unless %w[httpSample sample].include?(name)

      @level -= 1
      @parent_ts = nil if @level.zero?
    end
  end

  # Creates a virtual users distribution over time
  class VirtualUserTimeDist < Nokogiri::XML::SAX::Document
    attr_reader :vusers_matrix
    attr_reader :start_time

    def start_document
      @vusers_matrix = {} if @vusers_matrix.nil?
      @start_time = nil
      @host_matrix = {} if @host_matrix.nil?
    end

    def start_element(name, attrs = [])
      return unless %w[httpSample sample].include?(name)

      attrs_map = Hash[attrs]
      tms = attrs_map['ts'].to_i # ms
      ts = tms / 1000 # sec
      num_active_threads = attrs_map['na'].to_i

      if num_active_threads > 0
        host_name = attrs_map['hn']
        @host_matrix[ts] = [] if @host_matrix[ts].nil?
        if !@host_matrix[ts].include?(host_name)
          @vusers_matrix[ts] = @vusers_matrix[ts].to_i + num_active_threads
        elsif @vusers_matrix[ts].to_i < num_active_threads
          @vusers_matrix[ts] = num_active_threads
        end
        @host_matrix[ts].push(host_name) unless @host_matrix[ts].include?(host_name)
      end

      @start_time = tms if @start_time.nil? || (@start_time > tms)
    end
  end

  # Creates throughput distribution over time
  class ThroughputTimeDist < Nokogiri::XML::SAX::Document
    attr_reader :byte_matrix
    attr_reader :start_time

    def start_document
      @level = 0
      @byte_matrix = {} if @byte_matrix.nil?
    end

    def start_element(name, attrs = [])
      return unless %w[httpSample sample].include?(name)

      if @level.zero?
        attrs_map = Hash[attrs]
        tms = attrs_map['ts'].to_i # ms
        ts = tms / 1000 # sec
        @byte_matrix[ts] = @byte_matrix[ts].to_i + attrs_map['by'].to_i
        @start_time = tms if @start_time.nil? || (@start_time > tms)
      end
      @level += 1
    end

    def end_element(name)
      @level -= 1 if %w[httpSample sample].include?(name)
    end
  end

  # Builder factory of graphs
  module GraphBuilderFactory

    def self.aggregate_graph(identifier:, other_builder: nil)
      output_path = File.join(Hailstorm.root, Hailstorm.reports_dir, "aggregate_graph_#{identifier}")
      if other_builder
        other_builder.output_path = output_path
        other_builder
      else
        com.brickred.tsg.hailstorm.AggregateGraph.new(output_path)
      end
    end

    def self.time_series_graph(series_name:, range_name:, start_time:, other_builder: nil)
      if other_builder
        other_builder.series_name = series_name
        other_builder.range_name = range_name
        other_builder.start_time = start_time
        other_builder
      else
        com.brickred.tsg.hailstorm.TimeSeriesGraph.new(series_name, range_name, start_time)
      end
    end
  end

  private

  def build_threshold_matrix
    threshold_titles = []
    JSON.parse(self.page_stats.first.samples_breakup_json)
        .collect { |e| e['r'] }
        .each_with_index do |range, index|

      title = if !range.is_a?(Array)
                index.zero? ? "Under #{range}s" : "Over #{range}s"
              else
                "#{range.first}s to #{range.last}s"
              end
      threshold_titles.push(title)
    end

    threshold_data = self.page_stats
                         .collect(&:samples_breakup_json)
                         .collect { |json| JSON.parse(json).collect { |e| e['p'].to_f } }
                         .transpose
    [threshold_data, threshold_titles]
  end

  def response_time_matrix
    self.page_stats.collect do |e|
      [e.minimum_response_time, e.maximum_response_time,
       e.average_response_time, e.ninety_percentile_response_time,
       e.median_response_time]
    end.transpose
  end

  def set_defaults
    self.sample_response_times = Hailstorm::Support::Quantile.new
  end
end
