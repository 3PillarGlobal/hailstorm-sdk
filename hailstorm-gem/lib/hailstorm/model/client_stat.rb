require 'nokogiri'

require 'hailstorm/model'
require 'hailstorm/model/execution_cycle'
require 'hailstorm/model/jmeter_plan'
require 'hailstorm/model/page_stat'
require 'hailstorm/model/jtl_file'
require 'hailstorm/support/quantile'
require 'hailstorm/support/collection_helper'

# Statistics collected on the 'client' side.
# @author Sayantam Dey
class Hailstorm::Model::ClientStat < ActiveRecord::Base
  include Hailstorm::Support::CollectionHelper

  belongs_to :execution_cycle

  belongs_to :jmeter_plan

  belongs_to :clusterable, polymorphic: true

  has_many :page_stats, dependent: :destroy

  has_many :jtl_files, dependent: :delete_all

  # starting (minimum) timestamp of collected samples
  attr_accessor :start_timestamp

  # last sample collected
  attr_accessor :end_sample

  # Array to store 90% response time of all samples
  attr_accessor :sample_response_times

  after_initialize :set_defaults

  # Collects the statistics generated by the load generating cluster
  # @param [Hailstorm::Model::ExecutionCycle] execution_cycle
  # @param [Hailstorm::Behavior::Clusterable] cluster_instance
  def self.collect_client_stats(execution_cycle, cluster_instance)
    logger.debug { "#{self.class}.#{__method__}" }
    jmeter_plan_results_map = Hash.new { |h, k| h[k] = [] }
    result_mutex = Mutex.new
    local_log_path = File.join(Hailstorm.root, Hailstorm.log_dir)

    self.visit_collection(cluster_instance.master_agents.where(active: true)) do |master|
      result_file_name = master.result_for(self, local_log_path)
      result_file_path = File.join(local_log_path, result_file_name)
      result_mutex.synchronize do
        jmeter_plan_results_map[master.jmeter_plan_id].push(result_file_path)
      end
    end

    jmeter_plan_results_map.keys.sort.each do |jmeter_plan_id|
      self.create_client_stat(execution_cycle, jmeter_plan_id,
                              cluster_instance,
                              jmeter_plan_results_map[jmeter_plan_id])
    end
  end

  # create 1 record for client_stats if it does not exist yet
  # @param [Hailstorm::Model::ExecutionCycle] execution_cycle
  # @param [Integer] jmeter_plan_id
  # @param [Hailstorm::Behavior::Clusterable] clusterable
  # @param [Array<String>] stat_file_paths
  # @param [Boolean] rm_stat_file
  # @return [Hailstorm::Model::ClientStat]
  # TODO remove rm_stat_file and move the deletion of file(s) back to the caller (chain) where the files are created.
  def self.create_client_stat(execution_cycle, jmeter_plan_id, clusterable, stat_file_paths, rm_stat_file = true)
    jmeter_plan = Hailstorm::Model::JmeterPlan.find(jmeter_plan_id)
    template = ClientStatTemplate.new(jmeter_plan, execution_cycle, clusterable, stat_file_paths, rm_stat_file)
    template.logger = logger
    template.create
  end

  # @return [String] path to generated image
  def aggregate_graph
    page_labels = self.page_stats.collect(&:page_label)

    response_times = self.page_stats.collect do |e|
      [e.minimum_response_time, e.maximum_response_time,
       e.average_response_time, e.ninety_percentile_response_time,
       e.median_response_time]
    end.transpose

    threshold_titles = []
    JSON.parse(self.page_stats.first.samples_breakup_json)
        .collect { |e| e['r'] }
        .each_with_index do |range, index|

      title = if !range.is_a?(Array)
                if index == 0
                  "Under #{range}s"
                else
                  "Over #{range}s"
                end
              else
                "#{range.first}s to #{range.last}s"
              end
      threshold_titles.push(title)
    end

    threshold_data = self.page_stats
                         .collect(&:samples_breakup_json)
                         .collect { |json| JSON.parse(json).collect { |e| e['p'].to_f } }
                         .transpose

    error_percentages = self.page_stats.collect(&:percentage_errors)

    build_aggregate_graph(page_labels, response_times, threshold_titles, threshold_data, error_percentages)
  end

  def aggregate_stats
    self.page_stats.collect(&:stat_item)
  end

  def self.execution_comparison_graph(execution_cycles, width: 640, height: 600, grapher: nil)
    graph_path = execution_comparison_graph_path(execution_cycles)
    if grapher.nil?
      grapher_klass = com.brickred.tsg.hailstorm.ExecutionComparisonGraph
      grapher = grapher_klass.new(graph_path)
    else
      grapher.output_path = graph_path
    end

    # bug #Research-440
    # store the total_threads_count in a map such that if it is repeated for a
    # particular execution_cycle, the sequence Id is appended. This prevents the
    # points from collapsing in the graph.
    domain_labels = []
    execution_cycles.each do |execution_cycle|
      count_client_stats = 0
      total_ninety_percentile_response_time = 0.0
      total_transactions_per_second = 0.0

      execution_cycle.client_stats.each do |client_stat|
        count_client_stats += 1
        total_ninety_percentile_response_time += client_stat.aggregate_ninety_percentile
        total_transactions_per_second += client_stat.aggregate_response_throughput
      end

      execution_cycle_response_time = (total_ninety_percentile_response_time.to_f /
          count_client_stats).round(2)

      domain_label = execution_cycle.total_threads_count.to_s
      if domain_labels.include?(domain_label) # repeated total_threads_count(domain_label)
        domain_label.concat("-#{execution_cycle.id}")
      end
      domain_labels.push(domain_label)

      grapher.addResponseTimeDataItem(domain_label, execution_cycle_response_time)

      execution_cycle_throughput = (total_transactions_per_second.to_f / count_client_stats).round(2)
      grapher.addThroughputDataItem(domain_label, execution_cycle_throughput)
    end

    grapher.build(width, height) # <-- returns path to generated image
  end

  def collect_sample(sample)
    sample_timestamp = sample['ts'].to_f

    # start_timestamp
    if self.start_timestamp.nil? || (sample_timestamp < self.start_timestamp)
      self.start_timestamp = sample_timestamp
    end

    # end_sample
    if self.end_sample.nil? || (sample_timestamp > self.end_sample['ts'].to_f)
      self.end_sample = sample
    end

    self.sample_response_times.push(sample['t'])
  end

  # @param [String] export_dir directory for exported files
  # @return [String] path to exported file
  def write_jtl(export_dir, append_id = false)
    require(self.clusterable_type.underscore)
    file_name = [self.clusterable.slug.gsub(/[\W\s]+/, '_'),
                 self.jmeter_plan.test_plan_name.gsub(/[\W\s]+/, '_')].join('-')
    file_name.concat("-#{self.id}") if append_id
    file_name.concat('.jtl')

    export_file = File.join(export_dir, file_name)
    Hailstorm::Model::JtlFile.export_file(self, export_file) unless File.exist?(export_file)
    export_file
  end

  # Generates a hits per second graph
  def self.hits_per_second_graph(execution_cycle, width: 640, height: 300, grapher: nil)
    sax_document = Nokogiri::XML::SAX::Document.new
    sax_document.class_eval do
      attr_reader :hit_matrix
      attr_reader :start_time

      def start_document
        @level = 0
        @hit_matrix = {} if @hit_matrix.nil?
      end

      def start_element(name, attrs = [])
        if %w[httpSample sample].include?(name)
          @level += 1
          attrs_map = Hash[attrs]
          tms = attrs_map['ts'].to_i # ms
          ts = tms / 1000 # sec
          if @parent_ts.nil? || (@parent_ts != ts)
            @hit_matrix[ts] = @hit_matrix[ts].to_i + 1
          end
          @parent_ts = ts if @level == 1
          @start_time = tms if @start_time.nil? || (@start_time > tms)
        end
      end

      def end_element(name)
        if %w[httpSample sample].include?(name)
          @level -= 1
          @parent_ts = nil if @level == 0
        end
      end
    end
    sax_parser = Nokogiri::XML::SAX::Parser.new(sax_document)
    execution_cycle.client_stats.each do |client_stat|
      export_file = client_stat.write_jtl(Hailstorm.tmp_path, true)
      File.open(export_file, 'r') do |file|
        sax_parser.parse(file)
      end
    end

    grapher ||= com.brickred.tsg.hailstorm.TimeSeriesGraph.new('Requests/second', 'Requests', sax_document.start_time)
    sax_document.hit_matrix.each do |key, value|
      grapher.addDataPoint(key, value)
    end

    grapher.build(File.join(Hailstorm.root, Hailstorm.reports_dir, "hits_per_second_graph_#{execution_cycle.id}"),
                  width, height)
  end

  def self.active_threads_over_time_graph(execution_cycle, width: 640, height: 300, grapher: nil)
    sax_document = Nokogiri::XML::SAX::Document.new
    sax_document.class_eval do
      attr_reader :vusers_matrix
      attr_reader :start_time

      def start_document
        @vusers_matrix = {} if @vusers_matrix.nil?
        @start_time = nil
        @host_matrix = {} if @host_matrix.nil?
      end

      def start_element(name, attrs = [])
        if %w[httpSample sample].include?(name)
          attrs_map = Hash[attrs]
          tms = attrs_map['ts'].to_i # ms
          ts = tms / 1000 # sec
          num_active_threads = attrs_map['na'].to_i

          if num_active_threads > 0
            host_name = attrs_map['hn']
            @host_matrix[ts] = [] if @host_matrix[ts].nil?
            if !@host_matrix[ts].include?(host_name)
              @vusers_matrix[ts] = @vusers_matrix[ts].to_i + num_active_threads
            elsif @vusers_matrix[ts].to_i < num_active_threads
              @vusers_matrix[ts] = num_active_threads
            end
            @host_matrix[ts].push(host_name) unless @host_matrix[ts].include?(host_name)
          end

          @start_time = tms if @start_time.nil? || (@start_time > tms)
        end
      end
    end
    sax_parser = Nokogiri::XML::SAX::Parser.new(sax_document)

    execution_cycle.client_stats.each do |client_stat|
      export_file = client_stat.write_jtl(Hailstorm.tmp_path, true)
      File.open(export_file, 'r') do |file|
        sax_parser.parse(file)
      end
    end

    grapher ||= com.brickred.tsg.hailstorm.TimeSeriesGraph.new('Virtual Users / Second', 'Virtual Users',
                                                               sax_document.start_time)
    ts_keys = sax_document.vusers_matrix.keys.sort_by(&:to_i)
    ts_keys.each_with_index do |ts, index|
      previous_index = index > 1 ? index - 1 : index
      next_index = index < ts_keys.size - 1 ? index + 1 : index
      previous_count = sax_document.vusers_matrix[ts_keys[previous_index]]
      threads_count = sax_document.vusers_matrix[ts]
      next_count = sax_document.vusers_matrix[ts_keys[next_index]]
      if (previous_count == next_count) && (previous_count > threads_count)
        threads_count = previous_count # dip correction
      end
      grapher.addDataPoint(ts, threads_count)
    end
    grapher.build(File.join(Hailstorm.root, Hailstorm.reports_dir, "vusers_per_second_graph_#{execution_cycle.id}"),
                  width, height)
  end

  def self.throughput_over_time_graph(execution_cycle, width: 640, height: 300, grapher: nil)
    sax_document = Nokogiri::XML::SAX::Document.new
    sax_document.class_eval do
      attr_reader :byte_matrix
      attr_reader :start_time

      def start_document
        @level = 0
        @byte_matrix = {} if @byte_matrix.nil?
      end

      def start_element(name, attrs = [])
        return unless %w[httpSample sample].include?(name)
        if @level == 0
          attrs_map = Hash[attrs]
          tms = attrs_map['ts'].to_i # ms
          ts = tms / 1000 # sec
          @byte_matrix[ts] = @byte_matrix[ts].to_i + attrs_map['by'].to_i
          @start_time = tms if @start_time.nil? || (@start_time > tms)
        end
        @level += 1
      end

      def end_element(name)
        @level -= 1 if %w[httpSample sample].include?(name)
      end
    end
    sax_parser = Nokogiri::XML::SAX::Parser.new(sax_document)
    execution_cycle.client_stats.each do |client_stat|
      export_file = client_stat.write_jtl(Hailstorm.tmp_path, true)
      File.open(export_file, 'r') do |file|
        sax_parser.parse(file)
      end
    end

    grapher ||= com.brickred.tsg.hailstorm.TimeSeriesGraph.new('Throughput over time', 'Bytes Transferred',
                                                               sax_document.start_time)
    sax_document.byte_matrix.each do |key, value|
      grapher.addDataPoint(key, value)
    end

    grapher.build(File.join(Hailstorm.root, Hailstorm.reports_dir, "throughput_per_second_graph_#{execution_cycle.id}"),
                  width, height)
  end

  def first_sample_at
    Time.at((self.start_timestamp / 1000).to_i) if self.start_timestamp
  end

  # Receives event callbacks as XML is parsed
  class JtlDocument < Nokogiri::XML::SAX::Document
    attr_reader :page_stats_map

    def initialize(stat_klass, client_stat)
      @stat_klass = stat_klass
      @client_stat = client_stat
      @page_stats_map = {}
      @level = 0
    end

    # @overrides Nokogiri::XML::SAX::Document#start_element()
    # @param [String] name
    # @param [Array] attrs
    def start_element(name, attrs = [])
      # check for level 1 because we don't collect sub-samples
      if (@level == 1) && %w[httpSample sample].include?(name)
        attrs_map = Hash[attrs] # convert array of 2 element arrays to Hash
        label = attrs_map['lb'].strip
        unless @page_stats_map.key?(label)
          @page_stats_map[label] = @stat_klass.new(page_label: label, client_stat_id: @client_stat.id)
        end
        @client_stat.collect_sample(attrs_map)
        @page_stats_map[label].collect_sample(attrs_map)
      end
      @level += 1
    end

    # @overrides Nokogiri::XML::SAX::Document#end_element()
    def end_element(_name)
      @level -= 1
    end
  end

  # Template for creating client_stat
  class ClientStatTemplate
    attr_reader :jmeter_plan, :execution_cycle, :clusterable, :stat_file_paths, :rm_stat_file
    attr_accessor :logger

    def initialize(new_jmeter_plan, new_execution_cycle, new_clusterable, new_stat_file_paths, new_rm_stat_file)
      @jmeter_plan = new_jmeter_plan
      @execution_cycle = new_execution_cycle
      @clusterable = new_clusterable
      @stat_file_paths = new_stat_file_paths
      @rm_stat_file = new_rm_stat_file
    end

    # @return [Hailstorm::Model::ClientStat]
    def create
      stat_file_path = collate_stats
      client_stat = nil
      File.open(stat_file_path, 'r') do |file_io|
        client_stat = do_create_client_stat(file_io)
      end

      persist_jtl(client_stat, stat_file_path)
      File.unlink(stat_file_path) if rm_stat_file

      client_stat
    end

    private

    def collate_stats
      stat_file_path = stat_file_paths.first if stat_file_paths.size == 1
      stat_file_path || combine_stats
    end

    # Combines two or more JTL files to create new JTL file with combined stats.
    def combine_stats
      xml_decl = '<?xml version="1.0" encoding="UTF-8"?>'
      test_results_start_tag = '<testResults version="1.2">'
      test_results_end_tag = '</testResults>'
      file_unique_ids = [execution_cycle, jmeter_plan, clusterable].compact.map(&:id)
      combined_file_path = File.join(Hailstorm.tmp_path, "results-#{file_unique_ids.join('-')}-all.jtl")

      return combined_file_path if File.exist?(combined_file_path)

      File.open(combined_file_path, 'w') do |combined_file|
        combined_file.puts xml_decl
        combined_file.puts test_results_start_tag

        stat_file_paths.each do |file_path|
          File.open(file_path, 'r') do |file|
            file.each_line do |line|
              if line[xml_decl].nil? && line[test_results_start_tag].nil? && line[test_results_end_tag].nil?
                combined_file.print(line)
              end
            end
          end
        end

        combined_file.puts test_results_end_tag
      end

      # remove individual files
      if rm_stat_file
        stat_file_paths.each do |file_path|
          File.unlink(file_path)
        end
      end

      combined_file_path
    end

    def do_create_client_stat(io_like)
      client_stat = execution_cycle
                        .client_stats
                        .where(jmeter_plan_id: jmeter_plan.id,
                               clusterable_id: clusterable.id,
                               clusterable_type: clusterable.class.name,
                               threads_count: jmeter_plan.latest_threads_count)
                        .first_or_create!

      # SAX parsing
      jtl_document = JtlDocument.new(Hailstorm::Model::PageStat, client_stat)
      jtl_parser = Nokogiri::XML::SAX::Parser.new(jtl_document)
      jtl_parser.parse(io_like)

      # save in db
      jtl_document.page_stats_map.values.each(&:save!)

      # update aggregates
      aggregate_samples_count = client_stat.page_stats.sum(:samples_count)
      test_duration = (client_stat.end_sample['ts'].to_f +
          client_stat.end_sample['t'].to_f - client_stat.start_timestamp) / 1000.to_f

      client_stat.aggregate_response_throughput = (aggregate_samples_count.to_f / test_duration)

      logger.debug { 'Calculating aggregate_ninety_percentile...' } if logger
      client_stat.aggregate_ninety_percentile = client_stat.sample_response_times.quantile(90)
      logger.debug { '... finished calculating aggregate_ninety_percentile' } if logger

      # this is the duration of the last sample sent, it is in milliseconds, so
      # we divide it by 1000
      client_stat.last_sample_at = Time.at((client_stat.end_sample['ts'].to_i + client_stat.end_sample['t'].to_i) / 1000)

      client_stat.save!
      client_stat
    end

    # persist file to db and remove file from fs
    def persist_jtl(client_stat, stat_file_path)
      logger.info { "Persisting #{stat_file_path} to DB..." } if logger
      Hailstorm::Model::JtlFile.persist_file(client_stat, stat_file_path)
    end
  end

  private

  # @return [String] path to generated image
  def build_aggregate_graph(page_labels, response_times, threshold_titles, threshold_data, error_percentages)
    grapher = com.brickred.tsg.hailstorm.AggregateGraph.new(aggregate_graph_path)
    grapher.setPages(page_labels)
        .setNinetyCentileResponseTimes(response_times[3])
        .setThresholdTitles(threshold_titles)
        .setThresholdData(threshold_data)
        .setErrorPercentages(error_percentages)
        .create
  end

  def aggregate_graph_path
    File.join(Hailstorm.root, Hailstorm.reports_dir, "aggregate_graph_#{self.id}")
  end

  def self.execution_comparison_graph_path(execution_cyles)
    start_id = execution_cyles.first.id
    end_id = execution_cyles.last.id
    File.join(Hailstorm.root, Hailstorm.reports_dir, "client_execution_comparison_graph_#{start_id}-#{end_id}")
  end

  def set_defaults
    self.sample_response_times = Hailstorm::Support::Quantile.new
  end
end
